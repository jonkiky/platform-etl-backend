package io.opentargets.etl.backend.drug_beta

import com.typesafe.scalalogging.LazyLogging
import org.apache.spark.sql.{Column, DataFrame, SparkSession}
import org.apache.spark.sql.functions._

/**
  * Class to process ChEMBL indications for incorporation into Drug.
  *
  * Output schema:
  *
  * {
  *   efo_id : str ,
  *   efo_label : str ,
  *   efo_uri : str ,
  *   max_phase_for_indication : int ,
  *   references : [
  *     {
  *       source: str ,
  *       urls: [str1, ..., strn],
  *       ids: [str1, ..., strn]
  *     }]
  *  }
  *
  */
class Indication(indicationsRaw: DataFrame, efoRaw: DataFrame)(implicit sparkSession: SparkSession) {
  import sparkSession.implicits._

  def processIndications: DataFrame = {
    ???
  }

  /**
    *
    * @param rawEfoData taken from the `disease` input data
    * @return dataframe of `efo_id`, `efo_label`, `efo_uri`
    */
  private def getEfoDataframe(rawEfoData: DataFrame): DataFrame = {
    val columnsOfInterest = Seq(("code", "efo_url"), ("label", "efo_label"))
    val df = rawEfoData.select(columnsOfInterest.map(_._1).map(col): _*)
      .withColumn("efo_id", Indication.splitAndTakeLastElement(col("code")))
    // rename columns
    columnsOfInterest.foldLeft(df)((d, names) => d.withColumnRenamed(names._1, names._2))
  }

}

object Indication extends Serializable with LazyLogging {
  /**
    * Split a column and return last element
    * @param x column of strings to split
    * @param pattern to use in split
    * @return last element from array generated by split. 
    */
  def splitAndTakeLastElement(x: Column, pattern: String = "/"): Column = {

    val strArr: Column = split(x, pattern)
    val ind = size(strArr).minus(1)
    strArr(ind)

  }
}
